<h1 id="table-of-contents">Table of Contents</h1>
<ol>
  <li><a href="#Introduction">Introduction</a></li>
  <li><a href="#Architecture">Architecture</a></li>
  <li><a href="#RII-interaction">Resource Isolation Interface interaction</a>
    <ol>
      <li><a href="#Setting-Memory-Requirement">CgroupManager</a></li>
      <li><a href="#Setting-Shared-Memory">DockerManager</a></li>
      <li><a href="#Setting-CPU-Requirement">RuncLibManager</a></li>
    </ol>
  </li>
  <li><a href="#Configuring-NUMA">Configuring NUMA</a></li>
</ol>

<div id="Introduction" />

<h2 id="introduction">Introduction</h2>

<p>Non Uniform Memory Access (<a href="https://www.cc.gatech.edu/~echow/ipcc/hpc-course/HPC-numa.pdf">NUMA</a>) is a new system architecture
where the hosts resources are grouped by cores and memory in NUMA zones. Storm supports isolating/pinning worker prcesses to specific
NUMA zones via the supervisor to take advantage of this resource isolation and avoid the penalty of using cross zone bus transfers</p>
<div id="Architecture" />

<h2 id="architecture">Architecture</h2>

<p>Once Storm supervisors are configured for NUMA (see section below) they now heartbeat multiple heartbeats - one for each NUMA zone.
Each of these NUMA supervisors have a supervisor id with the same prefixed supervisor id with the NUMA id differentiating them.
Nimbus, and by extension the scheduler, see these heartbeats and views the supervisor as multiple supervisors - one per configured NUMA zone. 
Nimbus schedules topologies and assignments accordingly. The supervisor reads all assignments with the prefixed assignments and then
pins each worker to the NUMA zone according to the numa id in the assignment. The pinning depends on the Resource Isolation Interface used
and is elaborated on in the following section.</p>

<div id="RII-Interaction" />

<h3 id="resource-isolation-interface-interaction">Resource Isolation Interface interaction</h3>

<p>Each implementation of the Resource Isolation Interface (RII) should now implement NUMA pinning. The following are the current/soon to be available
implementations</p>

<h4 id="cgroupmanager">CgroupManager</h4>

<p>The CgroupManager prefixes the worker launch command with the numactl command for Linux hosts-</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>numactl --cpunodebind=&lt;numaId&gt;&gt; --membind=&lt;numaId&gt; &lt;worker launch command&gt;
</code></pre></div></div>

<p>The worker is then bound to the NUMA zoneâ€™s CPU cores and memory zone.</p>
<h4 id="dockermanager">DockerManager</h4>

<p>Will be updated upon adding Docker support</p>

<h4 id="runclibmanager">RuncLibManager</h4>

<p>Will be updated upon adding Runc support</p>

<div id="Configuring NUMA" />

<h3 id="configuring-numa">Configuring NUMA</h3>

<p>In the Supervisor Config the following settings need to be set</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>supervisor.numa.meta:
    "0": # Numa zone id
        numa.cores: # Cores in NUMA zone (can be determined by using the numastat command)
            - 0
            - 1
            - 2
            - 3
            - 4
            - 5
            - 12
            - 13
            - 14
            - 15
            - 16

        numa.generic.resources.map: # Generic Resources in the zone to be used for generic resource scheduling (optional)
            network.resource.units: 50.0

        numa.memory.mb: 42461 # Size of memory zone
        numa.ports: # Ports to be assigned to workers pinned to the NUMA zone (this may include ports not specified in SUPERVISOR_SLOTS_PORTS
            - 6700
            - 6701
            - 6702
            - 6703
            - 6704
            - 6705
            - 6706
            - 6707
            - 6708
            - 6709
            - 6710
            - 6711
</code></pre></div></div>
