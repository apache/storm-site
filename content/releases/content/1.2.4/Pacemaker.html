<h3 id="introduction">Introduction</h3>
<p>Pacemaker is a storm daemon designed to process heartbeats from workers. As Storm is scaled up, ZooKeeper begins to become a bottleneck due to high volumes of writes from workers doing heartbeats. Lots of writes to disk and too much traffic across the network is generated as ZooKeeper tries to maintain consistency.</p>

<p>Because heartbeats are of an ephemeral nature, they do not need to be persisted to disk or synced across nodes; an in-memory store will do. This is the role of Pacemaker. Pacemaker functions as a simple in-memory key/value store with ZooKeeper-like, directory-style keys and byte array values.</p>

<p>The corresponding Pacemaker client is a plugin for the <code class="language-plaintext highlighter-rouge">ClusterState</code> interface, <code class="language-plaintext highlighter-rouge">org.apache.storm.pacemaker.pacemaker_state_factory</code>. Heartbeat calls are funneled by the <code class="language-plaintext highlighter-rouge">ClusterState</code> produced by <code class="language-plaintext highlighter-rouge">pacemaker_state_factory</code> into the Pacemaker daemon, while other set/get operations are forwarded to ZooKeeper.</p>

<hr />

<h3 id="configuration">Configuration</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">pacemaker.host</code> : (deprecated) The host that the Pacemaker daemon is running on</li>
  <li><code class="language-plaintext highlighter-rouge">pacemaker.servers</code> : The hosts that the Pacemaker daemons are running on - This supercedes <code class="language-plaintext highlighter-rouge">pacemaker.host</code></li>
  <li><code class="language-plaintext highlighter-rouge">pacemaker.port</code> : The port that Pacemaker will listen on</li>
  <li><code class="language-plaintext highlighter-rouge">pacemaker.max.threads</code> : Maximum number of threads Pacemaker daemon will use to handle requests.</li>
  <li><code class="language-plaintext highlighter-rouge">pacemaker.childopts</code> : Any JVM parameters that need to go to the Pacemaker. (used by storm-deploy project)</li>
  <li><code class="language-plaintext highlighter-rouge">pacemaker.auth.method</code> : The authentication method that is used (more info below)</li>
</ul>

<h4 id="example">Example</h4>

<p>To get Pacemaker up and running, set the following option in the cluster config on all nodes:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>storm.cluster.state.store: "org.apache.storm.pacemaker.pacemaker_state_factory"
</code></pre></div></div>

<p>The Pacemaker servers also need to be set on all nodes:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pacemaker.servers:
    - somehost.mycompany.com
    - someotherhost.mycompany.com
</code></pre></div></div>
<p>The pacemaker.host config still works for a single pacemaker, although it has been deprecated.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pacemaker.host: single_pacemaker.mycompany.com
</code></pre></div></div>

<p>And then start all of your daemons</p>

<p>(including Pacemaker):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ storm pacemaker
</code></pre></div></div>

<p>The Storm cluster should now be pushing all worker heartbeats through Pacemaker.</p>

<h3 id="security">Security</h3>

<p>Currently digest (password-based) and Kerberos security are supported. Security is currently only around reads, not writes. Writes may be performed by anyone, whereas reads may only be performed by authorized and authenticated users. This is an area for future development, as it leaves the cluster open to DoS attacks, but it prevents any sensitive information from reaching unauthorized eyes, which was the main goal.</p>

<h4 id="digest">Digest</h4>
<p>To configure digest authentication, set <code class="language-plaintext highlighter-rouge">pacemaker.auth.method: DIGEST</code> in the cluster config on the nodes hosting Nimbus and Pacemaker.
The nodes must also have <code class="language-plaintext highlighter-rouge">java.security.auth.login.config</code> set to point to a JAAS config file containing the following structure:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PacemakerDigest {
    username="some username"
    password="some password";
};
</code></pre></div></div>

<p>Any node with these settings configured will be able to read from Pacemaker.
Worker nodes need not have these configs set, and may keep <code class="language-plaintext highlighter-rouge">pacemaker.auth.method: NONE</code> set, since they do not need to read from the Pacemaker daemon.</p>

<h4 id="kerberos">Kerberos</h4>
<p>To configure Kerberos authentication, set <code class="language-plaintext highlighter-rouge">pacemaker.auth.method: KERBEROS</code> in the cluster config on the nodes hosting Nimbus and Pacemaker.
The nodes must also have <code class="language-plaintext highlighter-rouge">java.security.auth.login.config</code> set to point to a JAAS config.</p>

<p>The JAAS config on Nimbus must look something like this:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PacemakerClient {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    keyTab="/etc/keytabs/nimbus.keytab"
    storeKey=true
    useTicketCache=false
    serviceName="pacemaker"
    principal="nimbus@MY.COMPANY.COM";
};
                         
</code></pre></div></div>

<p>The JAAS config on Pacemaker must look something like this:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PacemakerServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab="/etc/keytabs/pacemaker.keytab"
   storeKey=true
   useTicketCache=false
   principal="pacemaker@MY.COMPANY.COM";
};
</code></pre></div></div>

<ul>
  <li>The client’s user principal in the <code class="language-plaintext highlighter-rouge">PacemakerClient</code> section on the Nimbus host must match the <code class="language-plaintext highlighter-rouge">nimbus.daemon.user</code> storm cluster config value.</li>
  <li>The client’s <code class="language-plaintext highlighter-rouge">serviceName</code> value must match the server’s user principal in the <code class="language-plaintext highlighter-rouge">PacemakerServer</code> section on the Pacemaker host.</li>
</ul>

<h3 id="fault-tolerance">Fault Tolerance</h3>

<p>Pacemaker runs as a single daemon instance, making it a potential Single Point of Failure.</p>

<p>If Pacemaker becomes unreachable by Nimbus, through crash or network partition, the workers will continue to run, and Nimbus will repeatedly attempt to reconnect. Nimbus functionality will be disrupted, but the topologies themselves will continue to run.
In case of partition of the cluster where Nimbus and Pacemaker are on the same side of the partition, the workers that are on the other side of the partition will not be able to heartbeat, and Nimbus will reschedule the tasks elsewhere. This is probably what we want to happen anyway.</p>

<h3 id="zookeeper-comparison">ZooKeeper Comparison</h3>
<p>Compared to ZooKeeper, Pacemaker uses less CPU, less memory, and of course no disk for the same load, thanks to lack of overhead from maintaining consistency between nodes.
On Gigabit networking, there is a theoretical limit of about 6000 nodes. However, the real limit is likely around 2000-3000 nodes. These limits have not yet been tested.
On a 270 supervisor cluster, fully scheduled with topologies, Pacemaker resource utilization was 70% of one core and nearly 1GiB of RAM on a machine with 4 <code class="language-plaintext highlighter-rouge">Intel(R) Xeon(R) CPU E5530 @ 2.40GHz</code> and 24GiB of RAM.</p>

<p>Pacemaker now supports HA. Multiple Pacemaker instances can be used at once in a storm cluster to allow massive scalability. Just include the names of the Pacemaker hosts in the pacemaker.servers config and workers and Nimbus will start communicating with them. They’re fault tolerant as well. The system keeps on working as long as there is at least one pacemaker left running - provided it can handle the load.</p>
